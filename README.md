# nlp-papers
NLP Papers to read

Seq2seq:

Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity https://arxiv.org/pdf/2101.03961v3.pdf
